#!/usr/bin/env python3
"""
æ¯•ä¸šå…¸ç¤¼ç…§ç‰‡æ‰¹é‡ä¸‹è½½è„šæœ¬
æ³¨æ„ï¼šè¯·ç¡®ä¿å·²è·å¾—åˆæ³•æˆæƒå¹¶éµå®ˆç½‘ç«™ä½¿ç”¨æ¡æ¬¾
"""

import requests
import json
import os
import time
import re
from urllib.parse import urlparse, parse_qs, unquote
from pathlib import Path
import hashlib
import hmac
from typing import List, Dict, Optional


class PhotoDownloader:
    def __init__(self, download_dir: str = "graduation_photos", debug: bool = False):
        self.session = requests.Session()
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(exist_ok=True)
        self.debug = debug
        self.access_token = None

        # è®¾ç½®è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Sec-Ch-Ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"macOS"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-site',
            'Origin': 'https://www.xxpie.com',
        })

        # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰- é¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›
        self.request_delay = 0.5

        # å›¾ç‰‡è´¨é‡é€‰æ‹©
        self.quality_options = {
            'thumbnail': 'url_thumbnail',      # ç¼©ç•¥å›¾
            'large500': 'url_large500',        # 500px
            'large800': 'url_large800',        # 800px
            'large1024': 'url_large1024',      # 1024px
            'large': 'url_large',              # 1920px
            'large1920': 'url_large1920',      # 2560px
            'origin': 'url_origin'             # åŸå›¾
        }

    def debug_print(self, message: str):
        """è°ƒè¯•è¾“å‡º"""
        if self.debug:
            print(f"[DEBUG] {message}")

    def extract_album_info(self, album_url: str) -> Dict[str, str]:
        """ä»ç›¸å†ŒURLä¸­æå–ç›¸å†Œä¿¡æ¯"""
        try:
            parsed_url = urlparse(album_url)
            query_params = parse_qs(parsed_url.query)

            album_id = query_params.get('id', [None])[0]
            no_watermark = query_params.get('nowatermark', [None])[0]

            if not album_id:
                raise ValueError("æ— æ³•ä»URLä¸­æå–album_id")

            return {
                'album_id': album_id,
                'no_watermark': no_watermark or '',
                'referer': album_url
            }
        except Exception as e:
            raise ValueError(f"è§£æç›¸å†ŒURLå¤±è´¥: {e}")

    def extract_access_token(self, html_content: str) -> Optional[str]:
        """ä»é¡µé¢å†…å®¹ä¸­æå–access token"""
        # æŸ¥æ‰¾å¯èƒ½åŒ…å«tokençš„æ¨¡å¼
        token_patterns = [
            r'"x-access-token":\s*"([^"]+)"',
            r'x-access-token["\']?\s*[:=]\s*["\']([^"\']+)["\']',
            r'accessToken["\']?\s*[:=]\s*["\']([^"\']+)["\']',
            r'token["\']?\s*[:=]\s*["\']([^"\']+)["\']',
        ]

        for pattern in token_patterns:
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                token = match.group(1)
                if len(token) > 50:  # JWT tokené€šå¸¸å¾ˆé•¿
                    self.debug_print(f"æ‰¾åˆ°access token: {token[:20]}...")
                    return token

        return None
        
    def get_album_page(self, album_url: str) -> Optional[str]:
        """è·å–ç›¸å†Œé¡µé¢å†…å®¹"""
        try:
            # è®¾ç½®æ­£ç¡®çš„referer
            self.session.headers.update({
                'Referer': album_url,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'
            })

            response = self.session.get(album_url)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"è·å–ç›¸å†Œé¡µé¢å¤±è´¥: {e}")
            return None

    def get_photos_from_api(self, album_info: Dict[str, str], access_token: str = None) -> List[Dict]:
        """é€šè¿‡APIè·å–ç…§ç‰‡åˆ—è¡¨"""
        all_photos = []
        page_no = 1
        page_size = 60

        # è®¾ç½®APIè¯·æ±‚å¤´
        api_headers = self.session.headers.copy()
        api_headers.update({
            'Accept': 'application/json, text/plain, */*',
            'Referer': album_info['referer'],
        })

        if access_token:
            api_headers['x-access-token'] = access_token
            self.debug_print(f"ä½¿ç”¨access token: {access_token[:20]}...")

        while True:
            try:
                # æ„å»ºAPI URL
                api_url = "https://int.xxpie.com/api/pm/queryAlbumItemsPgByDefaultSort"
                params = {
                    'album_id': album_info['album_id'],
                    'page_no': page_no,
                    'page_size': page_size,
                    'platform': 'H5'
                }

                # æ·»åŠ å¯é€‰å‚æ•°
                if album_info.get('no_watermark'):
                    params['no_watermark'] = album_info['no_watermark']

                # å°è¯•ä»ç›¸å†ŒURLä¸­æå–sub_album_id
                referer_params = parse_qs(urlparse(album_info['referer']).query)
                if 'sub_album_id' in referer_params:
                    params['sub_album_id'] = referer_params['sub_album_id'][0]

                self.debug_print(f"è¯·æ±‚API: {api_url}")
                self.debug_print(f"å‚æ•°: {params}")

                response = self.session.get(api_url, params=params, headers=api_headers)
                response.raise_for_status()

                data = response.json()
                self.debug_print(f"APIå“åº”çŠ¶æ€ç : {data.get('code', 'unknown')}")

                if data.get('code') != 0:
                    print(f"APIè¿”å›é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    break

                result = data.get('result', {})
                photos = result.get('photos', [])

                if not photos:
                    self.debug_print(f"ç¬¬{page_no}é¡µæ²¡æœ‰æ›´å¤šç…§ç‰‡")
                    break

                all_photos.extend(photos)
                print(f"è·å–ç¬¬{page_no}é¡µ: {len(photos)}å¼ ç…§ç‰‡")

                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢
                if len(photos) < page_size:
                    break

                page_no += 1
                time.sleep(0.2)  # APIè¯·æ±‚é—´éš”

            except requests.RequestException as e:
                print(f"APIè¯·æ±‚å¤±è´¥: {e}")
                break
            except json.JSONDecodeError as e:
                print(f"APIå“åº”è§£æå¤±è´¥: {e}")
                break

        return all_photos
    
    def extract_image_urls(self, html_content: str) -> List[Dict[str, str]]:
        """ä»é¡µé¢å†…å®¹ä¸­æå–å›¾ç‰‡URL"""
        image_info = []

        # æ–¹æ³•1ï¼šæŸ¥æ‰¾å¯èƒ½åŒ…å«å›¾ç‰‡ä¿¡æ¯çš„JSONæ•°æ®
        json_patterns = [
            r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
            r'window\.albumData\s*=\s*({.+?});',
            r'var\s+albumData\s*=\s*({.+?});',
            r'photoList\s*:\s*(\[.+?\])',
        ]

        for pattern in json_patterns:
            json_match = re.search(pattern, html_content, re.DOTALL)
            if json_match:
                try:
                    data_str = json_match.group(1)
                    data = json.loads(data_str)

                    # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
                    photo_lists = []
                    if isinstance(data, dict):
                        # æŸ¥æ‰¾å¯èƒ½çš„ç…§ç‰‡åˆ—è¡¨
                        for key in ['photos', 'photoList', 'images', 'list', 'data']:
                            if key in data and isinstance(data[key], list):
                                photo_lists.append(data[key])

                        # é€’å½’æŸ¥æ‰¾åµŒå¥—ç»“æ„
                        def find_photo_arrays(obj, depth=0):
                            if depth > 3:  # é™åˆ¶é€’å½’æ·±åº¦
                                return
                            if isinstance(obj, dict):
                                for v in obj.values():
                                    if isinstance(v, list) and len(v) > 0:
                                        # æ£€æŸ¥æ˜¯å¦åƒç…§ç‰‡æ•°ç»„
                                        if isinstance(v[0], dict) and any(k in str(v[0]) for k in ['url', 'src', 'image']):
                                            photo_lists.append(v)
                                    elif isinstance(v, (dict, list)):
                                        find_photo_arrays(v, depth + 1)
                            elif isinstance(obj, list):
                                for item in obj:
                                    find_photo_arrays(item, depth + 1)

                        find_photo_arrays(data)
                    elif isinstance(data, list):
                        photo_lists.append(data)

                    # å¤„ç†æ‰¾åˆ°çš„ç…§ç‰‡åˆ—è¡¨
                    for photo_list in photo_lists:
                        for photo in photo_list:
                            if isinstance(photo, dict):
                                # æŸ¥æ‰¾URLå­—æ®µ
                                url = None
                                name = None

                                for url_key in ['url', 'src', 'image', 'imageUrl', 'photoUrl']:
                                    if url_key in photo and photo[url_key]:
                                        url = photo[url_key]
                                        break

                                for name_key in ['name', 'filename', 'title', 'originalName']:
                                    if name_key in photo and photo[name_key]:
                                        name = photo[name_key]
                                        break

                                if url and 'imagex.xxpie.com' in url:
                                    # ä»URLä¸­æå–æ–‡ä»¶åï¼ˆå¦‚æœæ²¡æœ‰æ‰¾åˆ°nameï¼‰
                                    if not name:
                                        parsed_url = urlparse(url)
                                        query_params = parse_qs(parsed_url.query)
                                        name = query_params.get('attname', [f'image_{len(image_info)+1}.jpg'])[0]

                                    image_info.append({
                                        'url': url,
                                        'name': name
                                    })

                    if image_info:
                        print(f"é€šè¿‡JSONæ•°æ®æ‰¾åˆ° {len(image_info)} å¼ å›¾ç‰‡")
                        return image_info

                except (json.JSONDecodeError, KeyError, TypeError) as e:
                    print(f"è§£æJSONæ•°æ®å¤±è´¥: {e}")
                    continue

        # æ–¹æ³•2ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾imagex.xxpie.comåŸŸåçš„å›¾ç‰‡é“¾æ¥
        print("å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å›¾ç‰‡é“¾æ¥...")
        url_patterns = [
            r'https://imagex\.xxpie\.com/[^"\s<>]+',
            r'"(https://imagex\.xxpie\.com/[^"]+)"',
            r"'(https://imagex\.xxpie\.com/[^']+)'",
        ]

        all_urls = set()
        for pattern in url_patterns:
            urls = re.findall(pattern, html_content)
            all_urls.update(urls)

        for url in all_urls:
            # ç¡®ä¿URLå®Œæ•´ä¸”åŒ…å«å¿…è¦å‚æ•°
            if 'attname=' in url and 'sign=' in url:
                parsed_url = urlparse(url)
                query_params = parse_qs(parsed_url.query)
                filename = query_params.get('attname', [f'image_{len(image_info)+1}.jpg'])[0]
                filename = unquote(filename)  # URLè§£ç 

                image_info.append({
                    'url': url,
                    'name': filename
                })

        # å»é‡
        seen_urls = set()
        unique_images = []
        for img in image_info:
            if img['url'] not in seen_urls:
                seen_urls.add(img['url'])
                unique_images.append(img)

        print(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ° {len(unique_images)} å¼ å›¾ç‰‡")
        return unique_images
    
    def download_image(self, image_url: str, filename: str, referer_url: str = None) -> bool:
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            # è®¾ç½®ä¸‹è½½è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¯·æ±‚
            headers = self.session.headers.copy()
            headers.update({
                'Referer': referer_url or 'https://www.xxpie.com/',
                'Origin': 'https://www.xxpie.com',
                'Priority': 'u=1, i',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-site',
            })

            print(f"æ­£åœ¨ä¸‹è½½: {filename}")
            print(f"URL: {image_url[:100]}...")

            response = self.session.get(image_url, headers=headers, stream=True, timeout=30)
            response.raise_for_status()

            # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '')
            if not content_type.startswith('image/'):
                print(f"è­¦å‘Š: å“åº”ä¸æ˜¯å›¾ç‰‡ç±»å‹ ({content_type})")

            # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸å®‰å…¨å­—ç¬¦
            safe_filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
            # ç¡®ä¿æ–‡ä»¶æœ‰æ‰©å±•å
            if '.' not in safe_filename:
                if 'jpeg' in content_type or 'jpg' in content_type:
                    safe_filename += '.jpg'
                elif 'png' in content_type:
                    safe_filename += '.png'
                else:
                    safe_filename += '.jpg'  # é»˜è®¤æ‰©å±•å

            filepath = self.download_dir / safe_filename

            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ£€æŸ¥å¤§å°
            if filepath.exists():
                existing_size = filepath.stat().st_size
                expected_size = int(response.headers.get('content-length', 0))
                if expected_size > 0 and existing_size == expected_size:
                    print(f"è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {safe_filename}")
                    return True
                else:
                    print(f"æ–‡ä»¶å·²å­˜åœ¨ä½†å¤§å°ä¸åŒ¹é…ï¼Œé‡æ–°ä¸‹è½½: {safe_filename}")

            # å†™å…¥æ–‡ä»¶
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0

            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)

                        # æ˜¾ç¤ºè¿›åº¦
                        if total_size > 0:
                            progress = (downloaded_size / total_size) * 100
                            print(f"\rè¿›åº¦: {progress:.1f}% ({downloaded_size}/{total_size} bytes)", end='', flush=True)

            print(f"\nä¸‹è½½å®Œæˆ: {safe_filename} ({downloaded_size} bytes)")
            return True

        except requests.RequestException as e:
            print(f"\nä¸‹è½½å¤±è´¥ {filename}: {e}")
            return False
        except IOError as e:
            print(f"\næ–‡ä»¶å†™å…¥å¤±è´¥ {filename}: {e}")
            return False
    
    def choose_quality(self) -> str:
        """é€‰æ‹©å›¾ç‰‡è´¨é‡"""
        print("\nè¯·é€‰æ‹©ä¸‹è½½å›¾ç‰‡è´¨é‡:")
        print("1. ç¼©ç•¥å›¾ (thumbnail) - æœ€å°æ–‡ä»¶")
        print("2. 500px (large500) - å°æ–‡ä»¶")
        print("3. 800px (large800) - ä¸­ç­‰æ–‡ä»¶")
        print("4. 1024px (large1024) - è¾ƒå¤§æ–‡ä»¶")
        print("5. 1920px (large) - å¤§æ–‡ä»¶")
        print("6. 2560px (large1920) - è¶…å¤§æ–‡ä»¶")
        print("7. åŸå›¾ (origin) - æœ€å¤§æ–‡ä»¶ï¼Œæœ€é«˜è´¨é‡")

        while True:
            choice = input("è¯·é€‰æ‹© (1-7ï¼Œé»˜è®¤é€‰æ‹©åŸå›¾): ").strip()
            if not choice:
                choice = "7"

            quality_map = {
                "1": "thumbnail",
                "2": "large500",
                "3": "large800",
                "4": "large1024",
                "5": "large",
                "6": "large1920",
                "7": "origin"
            }

            if choice in quality_map:
                selected_quality = quality_map[choice]
                print(f"å·²é€‰æ‹©: {selected_quality}")
                return selected_quality
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-7ä¹‹é—´çš„æ•°å­—")

    def download_album(self, album_url: str, quality: str = "origin") -> int:
        """ä¸‹è½½æ•´ä¸ªç›¸å†Œ"""
        print(f"å¼€å§‹å¤„ç†ç›¸å†Œ: {album_url}")

        try:
            # è§£æç›¸å†Œä¿¡æ¯
            album_info = self.extract_album_info(album_url)
            print(f"ç›¸å†ŒID: {album_info['album_id']}")

            # è·å–ç›¸å†Œé¡µé¢
            html_content = self.get_album_page(album_url)
            if not html_content:
                return 0

            # ä¿å­˜é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•
            debug_file = self.download_dir / "page_content.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°: {debug_file}")

            # å°è¯•æå–access token
            access_token = self.extract_access_token(html_content)
            if access_token:
                print("âœ… æ‰¾åˆ°è®¿é—®ä»¤ç‰Œ")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°è®¿é—®ä»¤ç‰Œï¼Œå°†å°è¯•æ— è®¤è¯è®¿é—®")

            # é€šè¿‡APIè·å–ç…§ç‰‡åˆ—è¡¨
            print("\næ­£åœ¨è·å–ç…§ç‰‡åˆ—è¡¨...")
            photos = self.get_photos_from_api(album_info, access_token)

            if not photos:
                print("âŒ æœªèƒ½è·å–åˆ°ç…§ç‰‡åˆ—è¡¨")
                print("å¯èƒ½çš„åŸå› :")
                print("1. éœ€è¦ç™»å½•è®¿é—®")
                print("2. ç›¸å†Œä¸å­˜åœ¨æˆ–å·²åˆ é™¤")
                print("3. ç½‘ç»œè¿æ¥é—®é¢˜")
                return 0

            print(f"âœ… æˆåŠŸè·å– {len(photos)} å¼ ç…§ç‰‡ä¿¡æ¯")

            # è½¬æ¢ä¸ºä¸‹è½½åˆ—è¡¨
            image_list = []
            quality_key = self.quality_options.get(quality, 'url_origin')

            for photo in photos:
                if quality_key in photo and photo[quality_key]:
                    image_list.append({
                        'url': photo[quality_key],
                        'name': photo.get('file_name', f"photo_{photo.get('gallery_ossobject_id', 'unknown')}.jpg"),
                        'size': photo.get('file_size', 0),
                        'width': photo.get('width', 0),
                        'height': photo.get('height', 0)
                    })

            if not image_list:
                print(f"âŒ æ²¡æœ‰æ‰¾åˆ°{quality}è´¨é‡çš„å›¾ç‰‡URL")
                return 0

            print(f"\nğŸ“Š å›¾ç‰‡ä¿¡æ¯ç»Ÿè®¡:")
            print(f"å›¾ç‰‡æ•°é‡: {len(image_list)}")
            print(f"é€‰æ‹©è´¨é‡: {quality}")

            # è®¡ç®—æ€»å¤§å°
            total_size = sum(img.get('size', 0) for img in image_list)
            if total_size > 0:
                print(f"é¢„è®¡æ€»å¤§å°: {total_size / 1024 / 1024:.1f} MB")

            # æ˜¾ç¤ºå‰å‡ å¼ å›¾ç‰‡ä¿¡æ¯
            print(f"\nå‰5å¼ å›¾ç‰‡é¢„è§ˆ:")
            for i, img in enumerate(image_list[:5]):
                size_info = f" ({img['size']/1024/1024:.1f}MB)" if img['size'] > 0 else ""
                resolution = f" {img['width']}x{img['height']}" if img['width'] > 0 else ""
                print(f"  {i+1}. {img['name']}{size_info}{resolution}")

            if len(image_list) > 5:
                print(f"  ... è¿˜æœ‰ {len(image_list) - 5} å¼ å›¾ç‰‡")

            # ç¡®è®¤ä¸‹è½½
            confirm = input(f"\nç¡®è®¤ä¸‹è½½è¿™ {len(image_list)} å¼ å›¾ç‰‡ï¼Ÿ(y/N): ").strip().lower()
            if confirm != 'y':
                print("å·²å–æ¶ˆä¸‹è½½")
                return 0

            # å¼€å§‹ä¸‹è½½
            return self.batch_download(image_list, album_url)

        except Exception as e:
            print(f"âŒ å¤„ç†ç›¸å†Œæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return 0


    def batch_download(self, image_list: List[Dict], referer_url: str) -> int:
        """æ‰¹é‡ä¸‹è½½å›¾ç‰‡"""
        success_count = 0
        failed_list = []

        print(f"\nå¼€å§‹æ‰¹é‡ä¸‹è½½ {len(image_list)} å¼ å›¾ç‰‡...")

        for i, image_info in enumerate(image_list, 1):
            print(f"\n[{i}/{len(image_list)}] ä¸‹è½½: {image_info['name']}")

            if self.download_image(image_info['url'], image_info['name'], referer_url):
                success_count += 1
            else:
                failed_list.append(image_info)

            # è¯·æ±‚é—´éš”
            if i < len(image_list):
                time.sleep(self.request_delay)

        print(f"\n{'='*50}")
        print(f"ä¸‹è½½å®Œæˆï¼æˆåŠŸä¸‹è½½ {success_count}/{len(image_list)} å¼ å›¾ç‰‡")

        if failed_list:
            print(f"\nå¤±è´¥çš„å›¾ç‰‡ ({len(failed_list)} å¼ ):")
            for img in failed_list:
                print(f"  - {img['name']}")

            # è¯¢é—®æ˜¯å¦é‡è¯•å¤±è´¥çš„å›¾ç‰‡
            retry = input("\næ˜¯å¦é‡è¯•å¤±è´¥çš„å›¾ç‰‡ï¼Ÿ(y/N): ").strip().lower()
            if retry == 'y':
                print("\nå¼€å§‹é‡è¯•å¤±è´¥çš„å›¾ç‰‡...")
                retry_success = 0
                for i, image_info in enumerate(failed_list, 1):
                    print(f"\n[é‡è¯• {i}/{len(failed_list)}] {image_info['name']}")
                    if self.download_image(image_info['url'], image_info['name'], referer_url):
                        retry_success += 1
                        success_count += 1
                    time.sleep(self.request_delay)

                print(f"\né‡è¯•å®Œæˆï¼é‡è¯•æˆåŠŸ {retry_success}/{len(failed_list)} å¼ å›¾ç‰‡")
                print(f"æ€»è®¡æˆåŠŸä¸‹è½½ {success_count}/{len(image_list)} å¼ å›¾ç‰‡")

        return success_count


def main():
    """ä¸»å‡½æ•°"""
    print("æ¯•ä¸šå…¸ç¤¼ç…§ç‰‡æ‰¹é‡ä¸‹è½½å·¥å…· v3.0 (APIç‰ˆæœ¬)")
    print("=" * 60)
    print("âœ¨ æ–°åŠŸèƒ½:")
    print("â€¢ ä½¿ç”¨å®˜æ–¹APIè·å–ç…§ç‰‡åˆ—è¡¨ï¼Œæ›´ç¨³å®šå¯é ")
    print("â€¢ æ”¯æŒå¤šç§å›¾ç‰‡è´¨é‡é€‰æ‹©")
    print("â€¢ æ˜¾ç¤ºæ–‡ä»¶å¤§å°å’Œåˆ†è¾¨ç‡ä¿¡æ¯")
    print("â€¢ æ”¹è¿›çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
    print("=" * 60)
    print("æ³¨æ„äº‹é¡¹ï¼š")
    print("1. è¯·ç¡®ä¿å·²è·å¾—å­¦æ ¡æˆ–ç…§ç‰‡æä¾›æ–¹çš„åˆæ³•æˆæƒ")
    print("2. è¯·éµå®ˆç½‘ç«™ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„å®š")
    print("3. è¯·åˆç†ä½¿ç”¨ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›")
    print("4. æœ¬å·¥å…·ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºå•†ä¸šç”¨é€”")
    print("=" * 60)

    # è·å–ç›¸å†ŒURL
    default_url = "https://www.xxpie.com/m/album?id=684fe6d7e66eb911b3071bc3&nowatermark=Njg0ZmU2ZDdlNjZlYjkxMWIzMDcxYmMzJDA=&mini=0"

    print(f"\né»˜è®¤ç›¸å†ŒURL:")
    print(f"{default_url}")

    while True:
        album_url = input(f"\nè¯·è¾“å…¥ç›¸å†Œé¡µé¢URL (ç›´æ¥æŒ‰å›è½¦ä½¿ç”¨é»˜è®¤URL): ").strip()

        # å¦‚æœç”¨æˆ·ç›´æ¥æŒ‰å›è½¦ï¼Œä½¿ç”¨é»˜è®¤URL
        if not album_url:
            album_url = default_url
            print(f"âœ… ä½¿ç”¨é»˜è®¤ç›¸å†ŒURL")
            break

        if not album_url.startswith('https://www.xxpie.com/m/album'):
            print("âŒ é”™è¯¯ï¼šè¯·æä¾›æœ‰æ•ˆçš„ç›¸å†ŒURL")
            print("URLåº”è¯¥ä»¥ 'https://www.xxpie.com/m/album' å¼€å¤´")
            print("ç¤ºä¾‹: https://www.xxpie.com/m/album?id=684fe6d7e66eb911b3071bc3&nowatermark=...")

            retry = input("æ˜¯å¦é‡æ–°è¾“å…¥ï¼Ÿ(y/N): ").strip().lower()
            if retry != 'y':
                print("å·²é€€å‡ºç¨‹åº")
                return
            continue

        print(f"âœ… ä½¿ç”¨è‡ªå®šä¹‰ç›¸å†ŒURL")
        break

    print(f"\nç›¸å†ŒURL: {album_url}")

    # è®¾ç½®ä¸‹è½½ç›®å½•
    default_dir = "graduation_photos"
    download_dir = input(f"è¯·è¾“å…¥ä¸‹è½½ç›®å½• (é»˜è®¤: {default_dir}): ").strip()
    if not download_dir:
        download_dir = default_dir

    print(f"ä¸‹è½½ç›®å½•: {download_dir}")

    try:
        # åˆ›å»ºä¸‹è½½å™¨
        downloader = PhotoDownloader(download_dir, debug=False)

        # é€‰æ‹©å›¾ç‰‡è´¨é‡
        quality = downloader.choose_quality()

        # ç¡®è®¤å¼€å§‹å¤„ç†
        confirm = input(f"\nç¡®è®¤å¼€å§‹å¤„ç†ç›¸å†Œï¼Ÿ(y/N): ").strip().lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆä¸‹è½½")
            return

        # å¼€å§‹ä¸‹è½½
        success_count = downloader.download_album(album_url, quality)

        if success_count > 0:
            print(f"\nğŸ‰ ä¸‹è½½å®Œæˆï¼å…±æˆåŠŸä¸‹è½½ {success_count} å¼ å›¾ç‰‡")
            print(f"ğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {download_dir}")
        else:
            print("\nâŒ æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•å›¾ç‰‡")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()